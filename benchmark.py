"""
PD-AdamW vs. Standard AdamW Benchmark
-------------------------------------
å¯¹æ¯”å®éªŒï¼š
1. Baseline: PyTorch åŸç”Ÿ AdamW + Cosine Annealing LR (å·¥ä¸šæ ‡å‡†)
2. PD-AdamW: è‡ªé€‚åº”æ­¥é•¿ä¼˜åŒ–å™¨ (æ— é¢„è®¾ Schedule)

è§‚å¯Ÿé‡ç‚¹ï¼š
åœ¨ä¸å‘Šè¯‰ PD-AdamW æ€»æ­¥æ•°çš„æƒ…å†µä¸‹ï¼Œå®ƒèƒ½å¦è‡ªåŠ¨æ‹Ÿåˆå‡ºç±»ä¼¼ Cosine çš„ä¸‹é™æ›²çº¿ï¼Ÿ
"""

import math
import time
import os
import requests
import torch
import torch.nn as nn
from torch.nn import functional as F

# ================= é…ç½®å‚æ•° =================
batch_size = 64
block_size = 128
max_iters = 1000       # ä¸¤ä¸ªé€‰æ‰‹éƒ½è·‘ 1000 æ­¥
learning_rate = 1e-3   # åˆå§‹ LR
device = 'cuda' if torch.cuda.is_available() else 'cpu'
n_embd = 128
n_head = 4
n_layer = 4
dropout = 0.1

print(f"Benchmark running on: {device}")

# ================= æ ¸å¿ƒç®—æ³•: PD-AdamW v2 =================
class PD_AdamW(torch.optim.Optimizer):
    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, 
                 weight_decay=1e-2, alpha=0.005, oscillation_penalty=2.0):
        # æ³¨æ„: alpha è®¾ä¸º 0.005 (æ¯” 0.01 ä¿å®ˆï¼Œæ¯” 0.002 æ¿€è¿›)
        defaults = dict(lr=lr, betas=betas, eps=eps, 
                        weight_decay=weight_decay, alpha=alpha,
                        oscillation_penalty=oscillation_penalty)
        super(PD_AdamW, self).__init__(params, defaults)

    @torch.no_grad()
    def step(self, closure=None):
        loss = None
        if closure is not None: loss = closure()

        # --- A. å…¨å±€ç»Ÿè®¡é‡ ---
        total_grad_diff_sq = 0.0
        total_param_diff_sq = 0.0
        total_dot_product = 0.0
        has_prev_state = False
        
        for group in self.param_groups:
            for p in group['params']:
                if p.grad is None: continue
                state = self.state[p]
                if 'prev_grad' in state and 'prev_param' in state:
                    has_prev_state = True
                    grad_diff = p.grad - state['prev_grad']
                    param_diff = p - state['prev_param']
                    total_grad_diff_sq += grad_diff.norm().item()**2
                    total_param_diff_sq += param_diff.norm().item()**2
                    total_dot_product += torch.sum(p.grad * state['prev_grad']).item()

        # --- B. ä¼°ç®— Lipschitz & éœ‡è¡æ£€æµ‹ ---
        suggested_lr = None
        if has_prev_state and total_param_diff_sq > 1e-12:
            L_k = math.sqrt(total_grad_diff_sq / total_param_diff_sq)
            
            # éœ‡è¡æƒ©ç½š: å¦‚æœæ¢¯åº¦åå‘ï¼Œå¼ºåˆ¶æ”¾å¤§æ›²ç‡ä¼°è®¡
            penalty = 1.0
            if total_dot_product < 0:
                penalty = group['oscillation_penalty']
            
            # è®¡ç®—å»ºè®®æ­¥é•¿
            alpha = group['alpha']
            suggested_lr = alpha / (L_k * penalty + 1e-8)

        # --- C. æ›´æ–°å‚æ•° ---
        active_lr = 0.0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            for p in group['params']:
                if p.grad is None: continue
                grad = p.grad
                state = self.state[p]

                if len(state) == 0:
                    state['step'] = 0
                    state['exp_avg'] = torch.zeros_like(p)
                    state['exp_avg_sq'] = torch.zeros_like(p)
                    state['prev_grad'] = torch.zeros_like(p)
                    state['prev_param'] = torch.zeros_like(p)
                    state['adaptive_lr'] = group['lr']

                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']
                state['step'] += 1

                p.mul_(1 - group['lr'] * group['weight_decay'])
                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)
                denom = exp_avg_sq.sqrt().add_(group['eps'])

                # åŠ¨æ€ LR æ›´æ–°
                current_lr = state['adaptive_lr']
                if suggested_lr is not None:
                    if suggested_lr > current_lr:
                        new_lr = current_lr * 1.05 # åŠ é€Ÿ
                    else:
                        new_lr = 0.9 * current_lr + 0.1 * suggested_lr # å‡é€Ÿ/åˆ¹è½¦
                    new_lr = max(min(new_lr, 0.05), 1e-6)
                    state['adaptive_lr'] = new_lr
                
                step_size = state['adaptive_lr']
                
                # Bias correction
                bias_correction1 = 1 - beta1 ** state['step']
                bias_correction2 = 1 - beta2 ** state['step']
                step_size = step_size * (math.sqrt(bias_correction2) / bias_correction1)
                
                # æ›´æ–°å‰ä¿å­˜çŠ¶æ€
                current_param_val = p.clone()
                state['prev_grad'].copy_(grad)
                state['prev_param'].copy_(current_param_val)

                p.addcdiv_(exp_avg, denom, value=-step_size)
                active_lr = state['adaptive_lr']
                
        return active_lr

# ================= è¾…åŠ©ä»£ç : æ•°æ®ä¸æ¨¡å‹ =================
# (ä¿æŒä¸å˜ï¼Œçœç•¥éƒ¨åˆ†ç»†èŠ‚ä»¥èŠ‚çœç¯‡å¹…ï¼ŒåŠŸèƒ½ä¸ä¹‹å‰ç›¸åŒ)
def get_data():
    if not os.path.exists('input.txt'):
        url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'
        with open('input.txt', 'w', encoding='utf-8') as f: f.write(requests.get(url).text)
    with open('input.txt', 'r', encoding='utf-8') as f: text = f.read()
    chars = sorted(list(set(text)))
    stoi = { ch:i for i,ch in enumerate(chars) }
    encode = lambda s: [stoi[c] for c in s]
    data = torch.tensor(encode(text), dtype=torch.long)
    n = int(0.9*len(data))
    return data[:n], data[n:], len(chars)

train_data, val_data, vocab_size = get_data()

def get_batch(split):
    data = train_data if split == 'train' else val_data
    ix = torch.randint(len(data) - block_size, (batch_size,))
    x = torch.stack([data[i:i+block_size] for i in ix])
    y = torch.stack([data[i+1:i+block_size+1] for i in ix])
    return x.to(device), y.to(device)

# ç®€åŒ–çš„æ¨¡å‹ç±»
class GPT(nn.Module):
    def __init__(self):
        super().__init__()
        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)
        self.position_embedding_table = nn.Embedding(block_size, n_embd)
        self.blocks = nn.Sequential(*[
            nn.TransformerEncoderLayer(d_model=n_embd, nhead=n_head, dim_feedforward=4*n_embd, 
                                       dropout=dropout, batch_first=True, norm_first=True) 
            for _ in range(n_layer)])
        self.ln_f = nn.LayerNorm(n_embd)
        self.lm_head = nn.Linear(n_embd, vocab_size)

    def forward(self, idx, targets=None):
        B, T = idx.shape
        x = self.token_embedding_table(idx) + self.position_embedding_table(torch.arange(T, device=device))
        x = self.blocks(x)
        x = self.ln_f(x)
        logits = self.lm_head(x)
        loss = None
        if targets is not None:
            loss = F.cross_entropy(logits.view(-1, vocab_size), targets.view(-1))
        return logits, loss

# ================= è®­ç»ƒå‡½æ•° =================
def run_training(optimizer_name):
    # æ¯æ¬¡é‡æ–°åˆå§‹åŒ–æ¨¡å‹ï¼Œä¿è¯å…¬å¹³
    torch.manual_seed(1337)
    model = GPT().to(device)
    
    if optimizer_name == "Baseline (AdamW+Cosine)":
        optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-2)
        # è¿™æ˜¯ä¸€ä¸ªå¼ºåŠ›çš„è¾…åŠ©ï¼šä½™å¼¦é€€ç«è°ƒåº¦å™¨
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_iters)
    else:
        # PD-AdamW è‡ªå¸¦å¯¼èˆªï¼Œä¸éœ€è¦ Scheduler
        optimizer = PD_AdamW(model.parameters(), lr=learning_rate, alpha=0.005, oscillation_penalty=2.0)
        scheduler = None

    print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ: {optimizer_name}")
    history = []
    start_t = time.time()
    
    for iter in range(max_iters):
        xb, yb = get_batch('train')
        logits, loss = model(xb, yb)
        
        optimizer.zero_grad(set_to_none=True)
        loss.backward()
        
        curr_lr = 0.0
        if optimizer_name == "PD-AdamW (AutoLR)":
            curr_lr = optimizer.step() # PD-AdamW è¿”å› LR
        else:
            optimizer.step()
            scheduler.step() # æ‰‹åŠ¨æ›´æ–° LR
            curr_lr = scheduler.get_last_lr()[0]

        if iter % 100 == 0 or iter == max_iters - 1:
            # å¿«é€ŸéªŒè¯ä¸€ä¸‹ Val Loss
            model.eval()
            with torch.no_grad():
                vx, vy = get_batch('val')
                _, vloss = model(vx, vy)
            model.train()
            
            print(f"Iter {iter:4d} | Train: {loss.item():.4f} | Val: {vloss.item():.4f} | LR: {curr_lr:.6f}")
            history.append((iter, loss.item(), vloss.item(), curr_lr))
            
    print(f"è€—æ—¶: {time.time()-start_time:.2f}s")
    return history

# ================= ä¸»ç¨‹åº =================
if __name__ == '__main__':
    start_time = time.time()
    
    # 1. è¿è¡ŒåŸºå‡† (Baseline)
    hist_base = run_training("Baseline (AdamW+Cosine)")
    
    # 2. è¿è¡Œå®éªŒç»„ (PD-AdamW)
    hist_pd = run_training("PD-AdamW (AutoLR)")

    # 3. æ‰“å°å¯¹æ¯”ç»“æœ
    print("\n" + "="*50)
    print("FINAL RESULT COMPARISON (Val Loss)")
    print("="*50)
    print(f"{'Iter':<6} | {'Baseline':<10} | {'PD-AdamW':<10} | {'Gap':<10}")
    print("-" * 50)
    
    for i in range(len(hist_base)):
        step, _, v_base, lr_base = hist_base[i]
        _, _, v_pd, lr_pd = hist_pd[i]
        
        diff = v_base - v_pd
        marker = "ğŸ† PD" if v_pd < v_base else "  Base"
        
        print(f"{step:<6} | {v_base:.4f}     | {v_pd:.4f}     | {marker}")

    print("="*50)
    print("åˆ†æ:")
    print("1. Baseline ä½¿ç”¨äº†å®Œç¾çš„ Cosine Schedule (å…ˆçƒ­èº«å†è¡°å‡)ã€‚")
    print("2. PD-AdamW å…¨ç¨‹è‡ªé€‚åº” (ä¸çŸ¥é“æ€»æ­¥æ•°)ã€‚")
    print("å¦‚æœ PD-AdamW çš„ Loss èƒ½æ¥è¿‘ç”šè‡³ä½äº Baselineï¼Œè¯´æ˜è‡ªé€‚åº”æœºåˆ¶æˆåŠŸäº†ã€‚")